{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación de reviews negativas y detección de tópicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Clasificación de Reviews**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jcbar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jcbar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importar librerías\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Descargar recursos necesarios de NLTK\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unir columnas \"*Summary*\" y \"*Text*\" en una sola: \"*Review*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los datos de entrenamiento y prueba\n",
    "train_path = \"Data/final-train.csv\"\n",
    "test_path = \"Data/test.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "# Unir las columnas \"Summary\" y \"Text\" en una sola columna llamada \"Review\"\n",
    "train_df['Review'] = train_df['Summary'].fillna('') + \" \" + train_df['Text'].fillna('')\n",
    "test_df['Review'] = test_df['Summary'].fillna('') + \" \" + test_df['Text'].fillna('')\n",
    "\n",
    "# Eliminar las columnas originales \"Summary\" y \"Text\"\n",
    "train_df = train_df.drop(columns=['Summary', 'Text'])\n",
    "test_df = test_df.drop(columns=['Summary', 'Text'])\n",
    "\n",
    "# Guardar los resultados en archivos CSV actualizados para los siguientes pasos\n",
    "train_df.to_csv(\"Data/train-processed.csv\", index=False)\n",
    "test_df.to_csv(\"Data/test-processed.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos de entrenamiento procesados:\n",
      "   Score                                             Review\n",
      "0      5  Can really notice when I am not drinking it I ...\n",
      "1      4  Okay, just not the best This is a decent balsa...\n",
      "2      3  goo source of fiber, not so much a source of c...\n",
      "3      5  Great GF Staple I have a 4 yr old and a 2 yr o...\n",
      "4      5  Coffee just doesn't taste right without Sweet ...\n",
      "\n",
      "Datos de prueba procesados:\n",
      "                                              Review\n",
      "0  I love Hills! My cat is picky, especially when...\n",
      "1  Thank you, Amazon! My mom, who has always beli...\n",
      "2  Good product, good price This is my third cont...\n",
      "3  Soooo Good! The first time I had a cup of this...\n",
      "4  Not as good as Libby's; cans in bad condition ...\n"
     ]
    }
   ],
   "source": [
    "# Imprimir las primeras filas para verificar el resultado\n",
    "print(\"Datos de entrenamiento procesados:\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\nDatos de prueba procesados:\")\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Para Random Forest:** \n",
    "### Preprocesamiento de datos: eliminación de stopwords, caracteres especiales, conversión a minúsculas, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener stopwords en inglés\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Función para limpiar el texto\n",
    "def clean_text_with_nltk(text):\n",
    "    # Convertir a minúsculas\n",
    "    text = text.lower()\n",
    "    # Eliminar caracteres especiales, números y puntuación\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Tokenizar el texto\n",
    "    words = nltk.word_tokenize(text)\n",
    "    # Eliminar stopwords\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Aplicar la limpieza a la columna 'Review' en los datos de entrenamiento y prueba\n",
    "train_df['Cleaned_Review'] = train_df['Review'].apply(clean_text_with_nltk)\n",
    "test_df['Cleaned_Review'] = test_df['Review'].apply(clean_text_with_nltk)\n",
    "\n",
    "# Guardar los datos con la nueva columna 'Cleaned_Review'\n",
    "train_df.to_csv(\"Data/train-cleaned.csv\", index=False)\n",
    "test_df.to_csv(\"Data/test-cleaned.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos de entrenamiento con texto limpio (NLTK):\n",
      "                                              Review  \\\n",
      "0  Can really notice when I am not drinking it I ...   \n",
      "1  Okay, just not the best This is a decent balsa...   \n",
      "2  goo source of fiber, not so much a source of c...   \n",
      "3  Great GF Staple I have a 4 yr old and a 2 yr o...   \n",
      "4  Coffee just doesn't taste right without Sweet ...   \n",
      "\n",
      "                                      Cleaned_Review  \n",
      "0  really notice drinking hormone problems past c...  \n",
      "1  okay best decent balsamic glazereducationbr br...  \n",
      "2  goo source fiber much source chocoate cookie m...  \n",
      "3  great gf staple yr old yr old cant eat gluten ...  \n",
      "4  coffee doesnt taste right without sweet n low ...  \n",
      "\n",
      "Datos de prueba con texto limpio (NLTK):\n",
      "                                              Review  \\\n",
      "0  I love Hills! My cat is picky, especially when...   \n",
      "1  Thank you, Amazon! My mom, who has always beli...   \n",
      "2  Good product, good price This is my third cont...   \n",
      "3  Soooo Good! The first time I had a cup of this...   \n",
      "4  Not as good as Libby's; cans in bad condition ...   \n",
      "\n",
      "                                      Cleaned_Review  \n",
      "0  love hills cat picky especially comes dry food...  \n",
      "1  thank amazon mom always believed breakfast imp...  \n",
      "2  good product good price third container drink ...  \n",
      "3  soooo good first time cup tea accupuncturists ...  \n",
      "4  good libbys cans bad condition ongoing problem...  \n"
     ]
    }
   ],
   "source": [
    "# Imprimir las primeras filas para verificar el resultado\n",
    "print(\"Datos de entrenamiento con texto limpio (NLTK):\")\n",
    "print(train_df[['Review', 'Cleaned_Review']].head())\n",
    "\n",
    "print(\"\\nDatos de prueba con texto limpio (NLTK):\")\n",
    "print(test_df[['Review', 'Cleaned_Review']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicar TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corroborar que no se hayan generados valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores nulos en entrenamiento: 1\n",
      "Valores vacíos en entrenamiento (incluidos los nulos): 1\n",
      "\n",
      "Valores nulos en prueba: 0\n",
      "Valores vacíos en prueba (incluidos los nulos): 0\n"
     ]
    }
   ],
   "source": [
    "# Contar valores nulos en la columna Cleaned_Review\n",
    "train_null_count = train_df['Cleaned_Review'].isnull().sum()\n",
    "test_null_count = test_df['Cleaned_Review'].isnull().sum()\n",
    "\n",
    "# Contar valores vacíos (incluyendo espacios en blanco)\n",
    "train_empty_count = (train_df['Cleaned_Review'].fillna('').str.strip() == '').sum()\n",
    "test_empty_count = (test_df['Cleaned_Review'].fillna('').str.strip() == '').sum()\n",
    "\n",
    "# Imprimir resultados\n",
    "print(f\"Valores nulos en entrenamiento: {train_null_count}\")\n",
    "print(f\"Valores vacíos en entrenamiento (incluidos los nulos): {train_empty_count}\")\n",
    "\n",
    "print(f\"\\nValores nulos en prueba: {test_null_count}\")\n",
    "print(f\"Valores vacíos en prueba (incluidos los nulos): {test_empty_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminar valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar y manejar valores nulos en la columna Cleaned_Review\n",
    "train_df['Cleaned_Review'] = train_df['Cleaned_Review'].fillna('')\n",
    "test_df['Cleaned_Review'] = test_df['Cleaned_Review'].fillna('')\n",
    "\n",
    "# Eliminar registros que puedan estar vacíos tras el preprocesamiento\n",
    "train_df = train_df[train_df['Cleaned_Review'].str.strip() != '']\n",
    "test_df = test_df[test_df['Cleaned_Review'].str.strip() != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Inicializar el vectorizador TF-IDF\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=10000,  # Limitar a 10,000 términos más frecuentes\n",
    "    stop_words='english',  # Eliminar stopwords adicionales si quedaron\n",
    "    ngram_range=(1, 2),  # Incluir unigrams y bigrams\n",
    "    max_df=0.9,  # Excluir términos demasiado frecuentes\n",
    "    min_df=5  # Excluir términos muy raros\n",
    ")\n",
    "\n",
    "# Ajustar el vectorizador al conjunto de entrenamiento y transformar\n",
    "X_train_tfidf = tfidf.fit_transform(train_df['Cleaned_Review'])\n",
    "X_test_tfidf = tfidf.transform(test_df['Cleaned_Review'])\n",
    "\n",
    "# Guardar el mapeo entre palabras y sus índices para referencia\n",
    "feature_names = tfidf.get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos de entrenamiento TF-IDF shape: (426339, 10000)\n",
      "Datos de prueba TF-IDF shape: (7105, 10000)\n"
     ]
    }
   ],
   "source": [
    "# Imprimir el tamaño de los datos transformados\n",
    "print(f\"Datos de entrenamiento TF-IDF shape: {X_train_tfidf.shape}\")\n",
    "print(f\"Datos de prueba TF-IDF shape: {X_test_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Para modelo tipo BERT:** \n",
    "### No se necesita de un preprocesamiento adicional, solo tokenización con el tokenizador propio de BERT. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado el lenguaje informal observado en los reviews, probaremos primero con el modelo *bertweet-base*. Este fue entrenado específicamente con datos de twitter y se desempeña bien en datasets con textos informales. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1deaf234cc047e09082247cb41d8806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/558 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jcbar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\jcbar\\.cache\\huggingface\\hub\\models--vinai--bertweet-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa09fce4fead43df8c947d2af27dec52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/843k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a2003a64653494fbb1a1238c278763c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "bpe.codes:   0%|          | 0.00/1.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08e9476b9b1c49d1848e79aa109db34d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.91M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Cargar el tokenizador para bertweet-base\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=True)\n",
    "\n",
    "# Tokenizar los datos de entrenamiento\n",
    "train_tokens = tokenizer(\n",
    "    train_df['Review'].tolist(),  # Lista de textos a tokenizar\n",
    "    padding=True,                # Agregar padding a la longitud máxima\n",
    "    truncation=True,             # Truncar textos largos al límite máximo del modelo\n",
    "    max_length=128,              # Longitud máxima de los textos tokenizados\n",
    "    return_tensors=\"pt\"          # Regresar tensores de PyTorch\n",
    ")\n",
    "\n",
    "# Tokenizar los datos de prueba\n",
    "test_tokens = tokenizer(\n",
    "    test_df['Review'].tolist(),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=128,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo de tokens del conjunto de entrenamiento:\n",
      "\n",
      "tensor([    0,   427,   116,  1607,    64,     8,   155,    46,  2065,    18,\n",
      "            8,    36,   118, 49232,  1191,    16,     6,   648,   992,    15,\n",
      "          254,    13,   177,   235,   106,  6719,    13,  8785,    59,    41,\n",
      "          154,    50,    15,  5168,     8,   436,   357,    19,  2642,  2860,\n",
      "            9, 27786,   639, 10584, 18871,  2603, 27067, 10584, 18871,  2603,\n",
      "        27067,   126,    17,    63,    15,    23, 17786,    61,  6580,     4,\n",
      "            8,    36,   108,  2065,    33,    19,   163, 29187,    77,     8,\n",
      "         1555,   297,   119,   825,    50,    30,    11,  2096,    15,    93,\n",
      "           13,   167,   223,    50,  3749,    18,     9, 12292, 10130,    11,\n",
      "         4069,     4,     8,    56,  1607,    64,     8, 58297, 15700,   108,\n",
      "         2065,   987,     7,    23,  4741,   297,     9,    51,    50,    15,\n",
      "         1205,    13,     8,   174,   249,  5647,  2154,   639, 10584, 18871,\n",
      "         2603, 27067, 10584, 18871,  2603, 27067,    76,     2])\n",
      "\n",
      "Ejemplo de tokens del conjunto de prueba:\n",
      "\n",
      "tensor([    0,     8,    71, 60823,    12,   122,  1407,    17, 13597,   460,\n",
      "            7,  1458,    64,    18,   632,     9,  3082, 24159,     4,   278,\n",
      "           70,  9951,    82,   262, 16769,     9,   531,   337,    65,     8,\n",
      "         2650,    11,  3488, 10223,    97, 18899,    29,    43,    24,    92,\n",
      "        10994,   500,  4728,  1253,   783, 41407,     4,     8, 10265,    23,\n",
      "         1407,    53,  5578, 13247,     9,  8469, 63214,   591,  9712,    64,\n",
      "           97,  4397,     9, 27645,   835,  9731,     4,  1003,     8,  1707,\n",
      "            9,   333,    63,    15,   268,  2642,  1059, 23526,     7,    30,\n",
      "           80, 57648,  6989,     7,    13,   121,  2835,   283, 52346,    13,\n",
      "         6916,  6916,  6392,   650,     4,   365,    97, 20364,    29,    82,\n",
      "        60308,     7,    13,     6,   185,    97,    86,   531, 48255,  2330,\n",
      "         1747,     4,  7349,     6,  1068, 31882,  2136,     7, 33758,   672,\n",
      "        12473,   567,  5239,  4728,  2911,  4397,     9,     2])\n"
     ]
    }
   ],
   "source": [
    "# Imprimir algunos ejemplos para verificar\n",
    "print(\"Ejemplo de tokens del conjunto de entrenamiento:\\n\")\n",
    "print(train_tokens['input_ids'][0])  # IDs de tokens\n",
    "print(\"\\nEjemplo de tokens del conjunto de prueba:\\n\")\n",
    "print(test_tokens['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tuning de Bertweet\n",
    "\n",
    "#### Entrenamiento del modelo pre-entrenado BERT utilizando el dataset etiquetado (calificaciones 1-5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validación para ajustar hiperparámetros (learning rate, batch size, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento y validación de Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación con conjunto de prueba (Recall, F1-score y Matriz de Confusión)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generación de CSVs con etiquetas predichas: 1 (negativo; calificaciones 1 y 2) y 0 (no negativo; calificaciones 3-5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Análisis de Tópicos:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Del dataset de entrenamiento, extraer opiniones negativas (calificaciones 1 y 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generar embeddings (usar modelo BERT al que le hayamos hecho fine tuning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determinar la cantidad de clusters ideales utilizando silhouette_score y el criterio del codo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicar K-Means con el número óptimo de clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mostrar tópicos con ejemplos representativos (e.g., textos más cercanos al centroide de cada cluster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear nubes de palabras para cada cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
